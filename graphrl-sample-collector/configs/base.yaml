run_config:
  prompt_file: ???
  output_dir: ???

  seed: 42

  num_prompts_per_step: 32
  num_rollouts_per_prompt: 16
  num_batches: 200

env:
  math:
    num_workers: 4
    math_verify_impl: "dapo_math_verify"

cluster:
  gpus_per_node: 2
  num_nodes: 1


policy:
  model_name: "Qwen/Qwen3-4B-Instruct-2507"
  tokenizer:
    name: ${policy.model_name}

  precision: "bfloat16"
  max_total_sequence_length: ???

  generation:
    model_name: ${policy.model_name}
    backend: "vllm"

    max_new_tokens: ???

    temperature: 1.0
    top_p: 1.0
    top_k: null

    stop_token_ids: null
    stop_strings: null

    colocated:
      enabled: false
      resources:
        gpus_per_node: ${cluster.gpus_per_node}
        num_nodes: ${cluster.num_nodes}

    vllm_cfg:
      tensor_parallel_size: 1
      pipeline_parallel_size: 1
      expert_parallel_size: 1
      gpu_memory_utilization: 0.6

      max_model_len: ${policy.max_total_sequence_length}
      async_engine: false
      precision: ${policy.precision}

      enforce_eager: false
